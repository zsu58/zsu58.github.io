---
title: "[Apache Airflow] Airflow - Docker Ubuntu18.04"
layout: single
date: '8/03/2022'
toc: true
toc_sticky: true
toc_label: Table of Contents
categories:
  - AIRFLOW
tags:
  - AIRFLOW
  - DOCKER
---

---
### Airflow - Docker Ubuntu18.04
* Run Docker Ubuntu 18.04 Image
* Install Python
* Airflow Install & Configuration
* Commit & Run Docker Container
* SSHOperator
* SparkSubmitOperator

---

### Run Docker Ubuntu 18.04 Image
* Run Docker Ubuntu 18.04 Image

```bash
docker container run -it -p 8090:8080 -e LC_ALL=C.UTF-8 --name airflow ubuntu:18.04
```
---

### Install Python

```bash
# install needed packages
apt-get update -y
apt-get install -y curl
apt-get install -y build-essential
apt-get install -y zlib1g-dev
apt-get install -y libncurses5-dev
apt-get install -y libgdbm-dev
apt-get install -y libnss3-dev
apt-get install -y libssl-dev
apt-get install -y libreadline-dev
apt-get install -y libffi-dev
apt-get install -y libbz2-dev
apt-get install libsqlite3-dev

# install python
cd tmp
PYTHON_VERSION=3.8.10
curl -O https://www.python.org/ftp/python/${PYTHON_VERSION}/Python-${PYTHON_VERSION}.tgz
tar -zxvf Python-${PYTHON_VERSION}.tgz
cd Python-${PYTHON_VERSION}
./configure --enable-optimizations
make altinstall
PYTHON_VERSION2="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
ln -s /usr/local/bin/${PYTHON_VERSION2} /usr/local/bin/python3
curl -O https://bootstrap.pypa.io/get-pip.py
python3 get-pip.py
rm -rf ../*
rm -rf /var/lib/apt/lists/*
```
---

### Airflow Install & Configuration

```bash
# create user
adduser airflow
# password (enter)
airflow

# create & activate virtual environment
mkdir airflow
cd airflow
python3 -m venv .venv
source ./.venv/bin/activate

# install airflow
pip3 install --upgrade pip
pip3 install --upgrade setuptools
pip3 install --upgrade distlib
AIRFLOW_HOME=$(pwd)
AIRFLOW_VERSION=2.1.4
PYTHON_VERSION="$(python --version | cut -d " " -f 2 | cut -d "." -f 1-2)"
CONSTRAINT_URL="https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt"
pip3 install "apache-airflow==${AIRFLOW_VERSION}" --constraint "${CONSTRAINT_URL}"

# add in user airflow's .bashrc
export AIRFLOW_HOME=/home/airflow/airflow

# initialize meta store meta database needed by airflow
airflow db init
```
---

### Commit & Run Docker Container
* DBë¥¼ postgresë¡œ ë°”ê¿”ì£¼ëŠ”ê±´ [ğŸ”— ë§í¬](https://carl020958.github.io/airflow/airflow3/) ì°¸ê³ 
* ì™„ë£Œ í›„ ë‚˜ë¨¸ì§€ëŠ” localì˜ zshì—ì„œ ì§„í–‰

```bash
docker run -p 5432:5432 --name postgres1 -e POSTGRES_PASSWORD=1234 -d -v psql_data:/var/lib/postgresql/data postgres:13

# create airflownet and include postgres in the airflownet
docker network create airflownet
docker network connect airflownet postgres1

# commit container as image
docker commit airflow carl020958/ubuntu-airflow:18.04-2.1.4-psql

# execute command in the directory 'dags' (for bind-mount)
docker container run -it -p 8090:8080 --network airflownet -v $(pwd):/home/airflow/airflow/dags -e LC_ALL=C.UTF-8 --name airflow carl020958/ubuntu-airflow:18.04-2.1.4-psql

# ì‹œì‘
docker container start airflow
docker container exec -it airflow bash
```
---

### SSHOperator
* Spark Clusterì˜ Job ì‹¤í–‰ì‹œí‚¤ê¸° ìœ„í•œ SSHOperator ì„¤ì¹˜
* Spark ClusterëŠ” [ğŸ”— ë§í¬](https://github.com/carl020958/docker) ì°¸ê³ 
* ì¶”í›„, ë¹„ë°€ë²ˆí˜¸ê°€ ì•„ë‹Œ id_rsaë¥¼ ë°”íƒ•ìœ¼ë¡œ ssh ì—°ê²°í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ë°©ë²•ìœ¼ë¡œ ë³´ì„

```bash
docker container start airflow
docker container exec -it airflow bash

# install openssh-client
apt-get update
apt-get install openssh-client

# ssh connect to spark-master's root
ssh root@spark-master

# airflow virtual environment
su - airflow
cd airflow
source ./.venv/bin/activate
pip3 install apache-airflow-providers-ssh

# Connection in Airflow-Webserver
# Conn Id: ssh_spark
# Conn Type: SSH
# Host: spark-master
# Username: root
# Password: 1234
# Port:
# Extra:

# test
airflow tasks test kid_news_wordcount wordcount 2022-03-27
```
---

### SparkSubmitOperator
* ê²°ê³¼ì ìœ¼ë¡œ, Airflowì˜ Pythonì—ëŠ” Spark ìª½ Clusterì˜ Pythonê³¼ ë‹¤ë¥¸ í™˜ê²½ì´ì—¬ì„œ Jobì´ ì •ìƒ ì‘ë™í•˜ì§€ ì•ŠìŒ

```bash
docker container start airflow
docker container exec -it airflow bash

# install openjdk8 (in root)
apt-get update
apt-get install -y openjdk-8-jdk
# check
java-version

# add in user airflow's .bashrc
su - airflow
JAVA_HOME="/usr/lib/jvm/java-8-openjdk-arm64"
export PATH="$PATH:$JAVA_HOME/bin"
# don't forget
source .bashrc

# install packages for SparkSubmitOperator
pip3 install apache-airflow-providers-apache-spark

# Connection in Airflow-Webserver
# Conn Id: spark_standalone
# COnn Type: Spark
# Host: spark://spark-master
# Port: 7077
# Extra: {"queue": "root.default", "deploy_mode": "cluster", "spark_home":"/usr/bin/spark-3.1.2-bin-hadoop3.2", "spark_binary": "spark-submit", "namespace": "default"}

# test
airflow tasks test kid_news_wordcount wordcount 2022-03-12
```
---




### ref 
* [ğŸ”— SSH ì°¸ê³ ](https://kimjingo.tistory.com/71)
* [ğŸ”— Dockerfile SSH ì°¸ê³ ](https://stackoverflow.com/questions/27860506/openssh-server-doesnt-start-in-docker-container)
* [ğŸ”— SSHOperator ì°¸ê³ ](https://stackoverflow.com/questions/57700262/need-help-running-spark-submit-in-apache-airflow)
* [ğŸ”— SparkSubmitOperator ì°¸ê³ 1](https://stackoverflow.com/questions/53344285/is-there-a-way-to-submit-spark-job-on-different-server-running-master)