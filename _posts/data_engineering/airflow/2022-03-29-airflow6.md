---
title: "[Apache Airflow] SSHOperator & SparkSubmitOperator"
layout: single
date: '29/03/2022'
toc: true
toc_sticky: true
toc_label: Table of Contents
categories:
  - AIRFLOW
tags:
  - AIRFLOW
  - DOCKER
---

---
### SSHOperator & SparkSubmitOperator
* SSHOperator
* SparkSubmitOperator

---

### SSHOperator
* Spark Clusterì˜ Job ì‹¤í–‰ì‹œí‚¤ê¸° ìœ„í•œ SSHOperator ì„¤ì¹˜
* Spark ClusterëŠ” [ğŸ”— ë§í¬](https://github.com/zsu58/docker) ì°¸ê³ 
* ì¶”í›„, ë¹„ë°€ë²ˆí˜¸ê°€ ì•„ë‹Œ id_rsaë¥¼ ë°”íƒ•ìœ¼ë¡œ ssh ì—°ê²°í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì€ ë°©ë²•ìœ¼ë¡œ ë³´ì„
* * [ğŸ”— Source Code](https://github.com/zsu58/kid_news_wordcount/blob/main/airflow/dags/kid_news_wordcount.py)

```bash
docker container start airflow
docker container exec -it airflow bash

# install openssh-client
apt-get update
apt-get install openssh-client

# ssh connect to spark-master's root
ssh root@spark-master

# airflow virtual environment
su - airflow
cd airflow
source ./.venv/bin/activate
pip3 install apache-airflow-providers-ssh

# Connection in Airflow-Webserver
# Conn Id: ssh_spark
# Conn Type: SSH
# Host: spark-master
# Username: root
# Password: 1234
# Port:
# Extra:

# test
airflow tasks test kid_news_wordcount wordcount 2022-03-27
```
---

### SparkSubmitOperator
* ê²°ê³¼ì ìœ¼ë¡œ, Airflowì˜ Pythonì—ëŠ” Spark ìª½ Clusterì˜ Pythonê³¼ ë‹¤ë¥¸ í™˜ê²½ì´ì—¬ì„œ Jobì´ ì •ìƒ ì‘ë™í•˜ì§€ ì•ŠìŒ

```bash
docker container start airflow
docker container exec -it airflow bash

# install openjdk8 (in root)
apt-get update
apt-get install -y openjdk-8-jdk
# check
java-version

# add in user airflow's .bashrc
su - airflow
JAVA_HOME="/usr/lib/jvm/java-8-openjdk-arm64"
export PATH="$PATH:$JAVA_HOME/bin"
# don't forget
source .bashrc

# install packages for SparkSubmitOperator
pip3 install apache-airflow-providers-apache-spark

# Connection in Airflow-Webserver
# Conn Id: spark_standalone
# COnn Type: Spark
# Host: spark://spark-master
# Port: 7077
# Extra: {"queue": "root.default", "deploy_mode": "cluster", "spark_home":"/usr/bin/spark-3.1.2-bin-hadoop3.2", "spark_binary": "spark-submit", "namespace": "default"}

from airflow.providers.apache.spark.operators import spark_submit
from airflow.contrib.operators.spark_submit_operator import SparkSubmitOperator
...

...
mongo_jar1 = "/home/airflow/airflow/spark/jars/bson-4.0.5.jar,"
mongo_jar2 = "/home/airflow/airflow/spark/jars/mongo-spark-connector_2.12-3.0.1.jar,"
mongo_jar3 = "/home/airflow/airflow/spark/jars/mongodb-driver-core-4.0.5.jar,"
mongo_jar4 = "/home/airflow/airflow/spark/jars/mongodb-driver-sync-4.0.5.jar,"
mysql_jar = "/home/airflow/airflow/spark/jars/mysql-connector-java-8.0.21.jar"
...

...
wordcount = SparkSubmitOperator(
    task_id="wordcount",
    conn_id="spark_standalone",
    application="/home/airflow/airflow/spark/applications/word_count_dump.py",
    total_executor_cores="6",
    executor_cores="2",
    executor_memory="3072m",
    num_executors="3",
    name="spark-wordcount",
    verbose=False,
    driver_memory="2g",
    jars=mongo_jar1 + mongo_jar2 + mongo_jar3 + mongo_jar4 + mysql_jar,
    dag=dag
)
...

# test
airflow tasks test kid_news_wordcount wordcount 2022-03-12
```
---




### ref 
* [ğŸ”— SSHOperator ì°¸ê³ ](https://stackoverflow.com/questions/57700262/need-help-running-spark-submit-in-apache-airflow)
* [ğŸ”— SparkSubmitOperator ì°¸ê³ 1](https://stackoverflow.com/questions/53344285/is-there-a-way-to-submit-spark-job-on-different-server-running-master)