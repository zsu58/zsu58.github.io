---
title: "[Web Crawling] Crawling Children News by Scrapy"
layout: single
date: '31/1/2022'
toc: true
toc_sticky: true
toc_label: Table of Contents
categories:
  - WEB_CRAWLING
tags:
  - SCRAPY
  - PYTHON
---

---
### Web Crawling
* ê°€ìƒí™˜ê²½ ë° scrapy í”„ë¡œì íŠ¸ êµ¬ì¶•
* ì‚¬ì „ í™•ì¸
* Requests
* Requests í•œê¸€ ê¹¨ì§
* Code
    * items.py
    * spiders/kidnews_spider.py
    * pipelines.py
    * settings.py
* scrapy ì‹¤í–‰

---

### ê°€ìƒí™˜ê²½ ë° scrapy í”„ë¡œì íŠ¸ êµ¬ì¶•

```bash
# ê°€ìƒí™˜ê²½ êµ¬ì¶•
python3 -m venv venv
source venv/bin/activate

# python library ì„¤ì¹˜
pip3 install scrapy
pip3 install ipykernel
pip3 install pymongo
pip3 install requests

# jupyter notebook kernel êµ¬ì¶•
python3 -m ipykernel install --user --name venv --display-name "venv_scrapy"

# scrapy project ì‹œì‘
scrapy startproject kidnewscrawling
```
---

### ì‚¬ì „ í™•ì¸
* interested_url/robots.txt ë¥¼ í†µí•´ crawling ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    * ì˜ˆ) http://kid.chosun.com/robots.txt
    * ì•„ë˜ëŠ” crawling ë¶ˆê°€ëŠ¥í•œ ì˜ì—­
        * Disallow: /sv*
        * Disallow: /list*
        * Disallow: /cartoonlist*
        * Disallow: /priv*

---

### Requests
* Requests ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ xpath/css ì°¾ê¸°


```python
import scrapy
import requests
import re
request_object = requests.get('http://kid.chosun.com/site/data/html_dir/2022/01/31/2022013100293.html')

if 'charset' not in request_object.headers['content-type']:
    request_object.encoding = request_object.apparent_encoding

response_object = scrapy.Selector(request_object)

p = " ".join(response_object.xpath('//div[@class="Paragraph"]//div[not(contains(@class, "center_img"))]//text()').extract())
re.sub(r'[<ì‚¬ì§„>\xa0]', '', p).strip()
```




    'í˜¸ì£¼ ì •ë¶€ê°€ ì½”ì•Œë¼ ìƒíƒœê³„ë¥¼ ë³´ì „í•˜ê¸° ìœ„í•´ 400ì–µ ì›ì´ ë„˜ëŠ” ëˆì„ ìŸì•„ë¶“ê¸°ë¡œ í–ˆë‹¤. 29ì¼(í˜„ì§€ ì‹œê°) CNNì— ë”°ë¥´ë©´ í˜¸ì£¼ ì •ë¶€ëŠ” ì½”ì•Œë¼ ê°œì²´ ìˆ˜(æ•¸)ë¥¼ ë³´í˜¸í•˜ê¸° ìœ„í•´ ì•ìœ¼ë¡œ 4ë…„ê°„ 5000ë§Œ í˜¸ì£¼ë‹¬ëŸ¬(ì•½ 422ì–µ ì›)ë¥¼ íˆ¬ìí•œë‹¤. ìŠ¤ì½§ ëª¨ë¦¬ìŠ¨ í˜¸ì£¼ ì´ë¦¬ëŠ” â€œì½”ì•Œë¼ëŠ” ì„¸ê³„ì—ì„œ ë‘ë°›ëŠ” í˜¸ì£¼ì˜ ìƒì§•â€ì´ë¼ë©° â€œë‹¤ìŒ ì„¸ëŒ€ë¥¼ ìœ„í•´ ìµœì„ ì„ ë‹¤í•´ ì½”ì•Œë¼ë¥¼ ë³´í˜¸í•´ì•¼ í•œë‹¤â€ê³  ë°í˜”ë‹¤. ì •ë¶€ ì˜ˆì‚°ì€ ì½”ì•Œë¼ ì„œì‹ì§€ ë³µì›ê³¼ ì¹˜ë£Œ, ì—°êµ¬ ë“±ì— ìš©ëœë‹¤. ì§€ë‚œí•´ 9ì›” í˜¸ì£¼ ì½”ì•Œë¼ ì¬ë‹¨ì´ ë°œí‘œí•œ ë‚´ìš©ì— ë”°ë¥´ë©´ 2018ë…„ ì´í›„ ì‚°ë¶ˆ, ê°€ë­„, í† ì§€ ê°œê°„(é–‹å¢¾) ë“±ìœ¼ë¡œ í˜¸ì£¼ì— ì„œì‹í•˜ëŠ” ì½”ì•Œë¼ì˜ ì•½ 30ï¼…ê°€ ë¼ì¡Œë‹¤. íŠ¹íˆ 2019ë…„ ë‰´ìš°ìŠ¤ì›¨ì¼ì¦ˆì˜ ë•… 4ë§Œ8000ã¢ì„ íŒŒê´´í•œ ëŒ€í˜• ì‚°ë¶ˆì€ ì½”ì•Œë¼ ê°œì²´ ê°ì†Œì— ì¹˜ëª…ì ì´ì—ˆë‹¤. í˜„ì¬ ì½”ì•Œë¼ëŠ” ì„¸ê³„ ìµœëŒ€ í™˜ê²½ë³´í˜¸ ê¸°êµ¬ì¸ â€˜ì„¸ê³„ìì—°ë³´ì „ì—°ë§¹(IUCN)â€™ì´ ì •í•˜ëŠ” â€˜ì·¨ì•½ì¢…(ç¨®)â€™ ëª©ë¡ì— ì˜¬ë¼ìˆë‹¤. *ì½”ì•Œë¼ë¥¼ ë³´í˜¸í•˜ëŠ” ë°©ë²•ì„ ì •ë¦¬í•´ ì•Œë ¤ì£¼ì„¸ìš”.'



---

### Requests í•œê¸€ ê¹¨ì§
* `request.headers['content-type']` - text/htmlì¸ ê²½ìš° ì¸ì½”ë”© ë¬¸ì œ ë°œìƒ
* í•´ê²°
    * ë°©ë²•1 - request.encodingì„ ì§ì ‘ì ìœ¼ë¡œ ìˆ˜ì •
    * ë°©ë²•2 - request.encodingì„ request.apparent_encodingìœ¼ë¡œ ë³€í™˜


```python
request = requests.get('http://kid.chosun.com/list_kj.html?catid=1')

print(request.headers['content-type']) 
print(request.apparent_encoding)
print(request.encoding)

request = requests.get('http://kid.chosun.com/list_kj.html?catid=1')
# ë°©ë²• 1
# request.encoding = 'euc-kr'

# ë°©ë²• 2
if 'charset' not in request.headers['content-type']:
    request.encoding = request.apparent_encoding

request = scrapy.Selector(request)
```

    text/html
    CP949
    ISO-8859-1


---

### Code

#### items.py


```python
# -*- coding: utf-8 -*-

import scrapy


class KidNewScrawlingItem(scrapy.Item):
    # define the fields for your item here like:
    url = scrapy.Field() # ê¸°ì‚¬ ì›ë¬¸ URL
    title = scrapy.Field() # ì œëª©
    subtitle = scrapy.Field() # ë¶€ì œëª©
    author = scrapy.Field() # ê¸°ì
    date = scrapy.Field() # ë‚ ì§œ   
    article = scrapy.Field() # ê¸°ì‚¬ ë‚´ìš©
    img_path = scrapy.Field() # ê¸°ì‚¬ img ê²½ë¡œ
    source = scrapy.Field() # ì‹ ë¬¸ì‚¬
#     category = scrapy.Field() # ì¹´í…Œê³ ë¦¬
    pass
```

#### spiders/kidnews_spider.py


```python
# -*- coding: utf-8 -*-

import scrapy
from kidnewscrawling.items import KidNewScrawlingItem
import re

class KidNewsSpider(scrapy.Spider):
    name = "kidNewsCrawler"
    
    def start_requests(self):
        for page in range(1,3):
            url = 'http://kid.chosun.com/list_kj.html?catid=1&pn={page}'.format(page=page)
            yield scrapy.Request(url=url, callback=self.parse_url)
        
    def parse_url(self, response):
        # ì¸ì½”ë”© ë³€í™˜
        if response.encoding == 'cp1252':
            response = response.replace(encoding='euc-kr')
        
        default_url = 'http://kid.chosun.com'
        urls = list(map(lambda x: default_url + x, response.xpath('//div[@class="subject"]/a/@href').extract()))
        
        for url in urls:
            yield response.follow(url=url, callback = self.parse_article)
            
    def parse_article(self, response):
        item = KidNewScrawlingItem()
        
        # ê¸°ì‚¬ url
        item["url"] = response.url
        
        # ê¸°ì‚¬ ì œëª©
        item["title"] = response.xpath('//title/text()').extract_first()

        # ê¸°ì‚¬ ë¶€ì œëª©, ì—†ëŠ” ê²½ìš°ë„ ì¡´ì¬
        item["subtitle"] = response.xpath('//h3/text()').extract_first()

        # ê¸°ì, ê¸°ìê°€ ì•„ë‹Œ ê²½ìš°ë„ ì¡´ì¬
        author = response.xpath('//span[@class="author"]/text()').extract_first()
        item["author"] = re.sub(r'[\r\n\tì •ë¦¬=]', '', author).strip()

        # ì…ë ¥ë‚ ì§œ
        date = response.xpath('//span[@class="date"]/text()').extract_first()
        item["date"] = re.sub(r'[\r\n\t]', '', date).strip()[5:21]
        
        # ë³¸ë¬¸
        p = " ".join(response.xpath('//div[@class="Paragraph"]//text()[normalize-space() \
                                                    and not(ancestor::*/@class="center_img") \
                                                    and not(ancestor::*/@class="right_img")]').extract())
        item["article"] = re.sub(r'[\r\n\t<ì‚¬ì§„>\xa0]', '', p).strip()

        # ê¸°ì‚¬ ì²« ì‚¬ì§„ ì´ë¯¸ì§€ url
        item["img_path"] = response.xpath('//img[@id="artImg0"]/@src').extract_first()
        
        # ì‹ ë¬¸ì§€
        item["source"] = "ì–´ë¦°ì´ì¡°ì„ ì¼ë³´"

        yield item
```

#### settings.py

```python
BOT_NAME = 'kidnewscrawling'

SPIDER_MODULES = ['kidnewscrawling.spiders']
NEWSPIDER_MODULE = 'kidnewscrawling.spiders'

# í•œê¸€ê¹¨ì§
FEED_EXPORT_ENCODING = 'utf-8'

...

# Obey robots.txt rules
ROBOTSTXT_OBEY = False

# mongodb setting
LOG_LEVEL='ERROR'

ITEM_PIPELINES = {'kidnewscrawling.pipelines.MongoPipeline': 300,}

MONGO_URI = 'mongodb://root:1234@localhost:27017'
MONGO_DATABASE = 'newsDB'
```

#### pipelines.py


```python
# -*- coding: utf-8 -*-

import pymongo
from itemadapter import ItemAdapter

class MongoPipeline:

    collection_name = 'news'

    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get('MONGO_URI'),
            mongo_db=crawler.settings.get('MONGO_DATABASE', 'items')
        )

    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    def close_spider(self, spider):
        self.client.close()

    def process_item(self, item, spider):
        print(item["title"])
        self.db[self.collection_name].insert_one(ItemAdapter(item).asdict())
        return item
```

### scrapy ì‹¤í–‰
* spiders/kidnews_spider.pyì˜ í´ë˜ìŠ¤ ë‚´ ì´ë¦„

```bash
scrapy crawl kidNewsCrawler
```

### ref
* [ğŸ”— Scrapy ê³µì‹ë¬¸ì„œ-ëª½ê³ DB](https://docs.scrapy.org/en/latest/topics/item-pipeline.html)
* [ğŸ”— ì°¸ê³  ë¸”ë¡œê·¸1](https://excelsior-cjh.tistory.com/86)
* [ğŸ”— ì°¸ê³  ë¸”ë¡œê·¸2](https://dev-jacob.tistory.com/entry/Python-Requests-í•œê¸€-ê¹¨ì§-ë¬¸ì œ-í•´ê²°í•˜ê¸°)
